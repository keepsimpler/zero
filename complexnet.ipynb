{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fengwf/anaconda3/envs/zero/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from zero.imports import *\n",
    "from zero.core import *\n",
    "from zero.graph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks constructed based on DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NodeOP(nn.Module):\n",
    "    \"\"\"\n",
    "    The Operation of every inner node in the network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ni : number of input channels\n",
    "    no : number of output channels\n",
    "    nh : number of hidden channels\n",
    "    Unit : the operation at the node\n",
    "    kwargs : arguments into `Unit`\n",
    "    \"\"\"\n",
    "    def __init__(self, ni:int, no:int, nh:int, Unit:nn.Module, **kwargs):\n",
    "        super(NodeOP, self).__init__()\n",
    "        self.unit = Unit(ni, no, nh, **kwargs)\n",
    "            \n",
    "    def forward(self, *inputs):\n",
    "        sum_inputs = sum(inputs)        \n",
    "        out = self.unit(sum_inputs)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NetworkOP(nn.Module):\n",
    "    \"\"\"\n",
    "    The operations along a DAG network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G   :  the `NetworkX` 'DiGraph' object, represent a DAG.\n",
    "    ni  :  number of input channels of the network\n",
    "    no  :  number of output channel of the network\n",
    "    Unit : operation at every inner node\n",
    "    stride : whether downsample or not, at the end of the network\n",
    "    efficient : does using the checkpointing of `Pytorch` to save memory.\n",
    "    kwargs : arguments into `Unit`\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, G:nx.DiGraph, ni:int, no:int, Unit:nn.Module, stride:int=1,\n",
    "                 efficient:bool=False, **kwargs):\n",
    "        super(NetworkOP, self).__init__()\n",
    "        self.efficient = efficient\n",
    "        self.G = G\n",
    "        self.n = G.graph['n'] # number of nodes\n",
    "        self.nodeops = nn.ModuleList() # ! need improve, to Moduledict !\n",
    "        for id in G.nodes(): # for each node\n",
    "            if id == -1:  # if is the unique input node, do nothing\n",
    "                continue\n",
    "            elif id == self.n:  # if is the unique output node\n",
    "                # then, concat its predecessors\n",
    "                n_preds = len([*G.predecessors(id)])\n",
    "                self.nodeops += [IdentityMapping(n_preds * ni, no, stride=stride)]\n",
    "            else:  # if is the inner node\n",
    "                self.nodeops += [NodeOP(ni, ni, ni, Unit, **kwargs)]\n",
    "            \n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        results[-1] = x  # input data is the result of the unique input node\n",
    "        for id in self.G.nodes(): # for each node\n",
    "            if id == -1:  # if is the input node, do nothing\n",
    "                continue\n",
    "            # get the results of all predecessors\n",
    "            inputs = [results[pred]  for pred in self.G.predecessors(id)]\n",
    "            if id == self.n: # if is the output node\n",
    "                cat_inputs = torch.cat(inputs, dim=1) # concat results of all predecessors\n",
    "                if self.efficient:\n",
    "                    return cp.checkpoint(self.nodeops[id], cat_inputs) \n",
    "                else:\n",
    "                    return self.nodeops[id](cat_inputs)\n",
    "            else: # if is inner nodes\n",
    "                if self.efficient:\n",
    "                    results[id] = cp.checkpoint(self.nodeops[id], *inputs) \n",
    "                else:\n",
    "                    results[id] = self.nodeops[id](*inputs)\n",
    "\n",
    "            # 删除前驱结点result中，不再需要的result\n",
    "            for pred in self.G.predecessors(id):  # 获得节点的所有前驱结点\n",
    "                succs = list(self.G.successors(pred))  # 获得每个前驱结点的所有后继节点\n",
    "                # 如果排名最后的后继节点是当前节点，说明该前驱结点的result不再被后续的节点需要，可以删除\n",
    "                if max(succs) == id:  \n",
    "                    del results[pred]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ComplexNet(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Neural Network based on complex network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Gs  :  a list of `NetworkX DiGraph` objects.\n",
    "    ns  :  number of channels of all stages.\n",
    "    Stage : class of a network.\n",
    "    Unit : class of a node.\n",
    "    c_out : number of output channels of the whole neural network.\n",
    "    efficient : does using the checkpointing of `Pytorch` to save memory.\n",
    "    kwargs : additional args into `Unit` class\n",
    "    \"\"\"\n",
    "    def __init__(self, Gs:list, ns:list, Stage:nn.Module, Unit:nn.Module, c_out:int=10,\n",
    "                 efficient:bool=False, **kwargs):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        stem = conv_bn(3, ns[0])\n",
    "        network_ops = []\n",
    "        for i in range(len(ns)-2):\n",
    "            network_ops += [Stage(Gs[i], ns[i], ns[i+1], Unit, stride=2, efficient=efficient, **kwargs)]\n",
    "        # the last stage has stride=1 at its end, i.e. no downsampling\n",
    "        network_ops += [Stage(Gs[-1], ns[-2], ns[-1], Unit, stride=1, efficient=efficient, **kwargs)]\n",
    "        \n",
    "        classifier = Classifier(ns[-1], c_out)\n",
    "        super().__init__(\n",
    "            stem,\n",
    "            *network_ops,\n",
    "            classifier\n",
    "        )\n",
    "        init_cnn(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All  hyperparameters\n",
    "\n",
    "* Parameters about graph features:\n",
    "  1. random graph models\n",
    "    * ER model :\n",
    "    * BA model :\n",
    "    * PC (Powerlaw-Clustering) model :\n",
    "    * WS model :\n",
    "    * fitness model : \n",
    "    * cascade model : \n",
    "    * niche model :\n",
    "    * coherent model :\n",
    "  2. n,m,k,p\n",
    "* Parameters about neural networks:\n",
    "  1. number of stages, 4,5\n",
    "  2. number of nodes per stage, 50\n",
    "  3. channels per node, 32\n",
    "* Operation on node:\n",
    "  1. xception.\n",
    "  2. resnext blocks, from sparse to dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Schedule\n",
    "0. Fix operation on node: `xception`\n",
    "1. Fix parameters about neural networks.\n",
    "  * `num_stages` : [4, 5]\n",
    "  * `num_nodes` : [32,48,64]\n",
    "  * `num_channels` : [32,64]\n",
    "2. Chose a random graph model, then give coefficient values, then generate a DAG.\n",
    "3. Create neural network model based on the DAG.\n",
    "4. Training data and evaluate performance of the model.\n",
    "5. Repeat 2-5, until all the random graph models, their coefficients, and generated DAGs are traveled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def complexnet(Stage:nn.Module=NetworkOP, Unit:nn.Module=xception, option:str=None):\n",
    "    species = [50,50,50,50]\n",
    "    connectance = [0.05]*4\n",
    "    Gs = [after_DAG(niche(s, c), option=option) for s,c in zip(species, connectance)]\n",
    "    ns = [32]*(4+1)\n",
    "    model = ComplexNet(Gs, ns, Stage, Unit, efficient=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = complexnet_32(NetworkOP, xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(344298)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted complexnet.ipynb to zero/complexnet.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py complexnet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Transform DAG to Multiple Partitioned Network\n",
    "\n",
    "Directed Acyclic Graphs are partial order, many node groups have no links among their inner nodes, therefore,  the operations of nodes belong to the same group can be executed parallelly.\n",
    "We transform DAGs to such multiple partitioned networks, such that the nodes in the same partition can be executed parallelly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def to_multiple_partitions(G):\n",
    "    \"\"\"\n",
    "    Transform DAGs to Multiple Partitioned Networks.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    llinks : list of links among partitions\n",
    "    partitions : number of nodes in all partitions\n",
    "    \"\"\"\n",
    "    #first, convert `NetworkX` graph object to `Numpy` matrix order by node IDs\n",
    "    A = nx.to_numpy_matrix(G, nodelist=sorted(G.nodes))\n",
    "    hcur = 1 # horizontal cursor, initialized as 1\n",
    "    vcur = 0 # vertical cursor, initialized as 0\n",
    "    n = A.shape[0] # number of nodes\n",
    "    llinks = []  # initialize list of links as empty list \n",
    "    partitions = []  # initialize number of nodes in partitions\n",
    "    while vcur < n:\n",
    "        current_nodes = list(range(vcur, hcur)) # current nodes\n",
    "        # links to current partition  A[:vcur, current_nodes]\n",
    "        links = []\n",
    "        for row in range(vcur):\n",
    "            for col in current_nodes:\n",
    "                if A[row, col] == 1:\n",
    "                    links.append((row, col - vcur))\n",
    "        if current_nodes != [0]: # if not the start partition\n",
    "            llinks.append(links)\n",
    "            partitions.append(len(current_nodes))\n",
    "            \n",
    "        # move on vertical cursor\n",
    "        vcur += len(current_nodes)\n",
    "        # move on horizontal pointer\n",
    "        while hcur < n and all(A[vcur:, hcur]==0):\n",
    "            hcur += 1\n",
    "        #print(vcur, hcur)\n",
    "        \n",
    "    return llinks, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "llinks, partitions = to_multiple_partitions(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "llinks, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(llinks[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Neural networks constructed based on Multiple Partitioned Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Partition(nn.Module):\n",
    "    \"\"\"\n",
    "    The operation of one partition in the multiple partitioned networks.\n",
    "    One partition includes several nodes, which can be executed parallelly.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ni : number of input channels of ONE node.\n",
    "    cur_nodes : number of nodes in current partition.\n",
    "    links : links from the nodes of all previous partitions to the nodes of current partition.\n",
    "    has_groups: does group the nodes?\n",
    "    Unit : the operation at the node.\n",
    "    kwargs : arguments into `Unit`\n",
    "    \"\"\"\n",
    "    def __init__(self, ni:int, cur_nodes:int, links:list, has_groups:bool, Unit:nn.Module, **kwargs):\n",
    "        super(Partition, self).__init__()\n",
    "        self.ni, self.cur_nodes = ni, cur_nodes\n",
    "        self.links = torch.tensor(links)\n",
    "        if has_groups:\n",
    "            groups = self.cur_nodes\n",
    "            kwargs['groups'] = groups # add to `kwargs`\n",
    "        self.op = Unit(ni * self.cur_nodes, ni * self.cur_nodes, ni * self.cur_nodes, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # number of channels of input should be `number of nodes in all previous partitions * self.ni`\n",
    "        # number of channels of output should be `self.cur_nodes * self.ni`\n",
    "        # first construct\n",
    "        N,C,H,W = x.size()\n",
    "        y = x.new_zeros((N, self.cur_nodes * self.ni, H, W))\n",
    "        for i in range(self.links.size(0)):\n",
    "            y[:, self.links[i,1] * self.ni : (self.links[i,1]+1) * self.ni, :, :] += \\\n",
    "            x[:, self.links[i,0] * self.ni : (self.links[i,0]+1) * self.ni, :, :]\n",
    "        out = self.op(y)\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Stage(nn.Module):\n",
    "    \"\"\"\n",
    "    The operations along a multiple partitioned network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G   :  the `NetworkX` 'DiGraph' object, represent a DAG.\n",
    "    ni  :  number of input channels of one node for the network\n",
    "    no  :  number of output channel of one node for the network\n",
    "    Unit : operation at every inner node\n",
    "    stride : whether downsample or not, at the end of the network\n",
    "    efficient : does using the checkpointing of `Pytorch` to save memory.\n",
    "    has_groups: does group the nodes?\n",
    "    kwargs : arguments into `Unit`\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, G:nx.DiGraph, ni:int, no:int, Unit:nn.Module, stride:int=1,\n",
    "                 efficient:bool=False, has_groups:bool=True, **kwargs):\n",
    "        super(Stage, self).__init__()\n",
    "        self.efficient = efficient\n",
    "        llinks, partitions = to_multiple_partitions(G)\n",
    "        self.llinks, self.partitions = llinks, partitions\n",
    "        \n",
    "        self.nodeops = nn.ModuleList() # ! need improve, to Moduledict !\n",
    "        for i, links in enumerate(llinks):\n",
    "            if i == len(llinks) - 1:  # reach the end of list\n",
    "                n_preds = sum(partitions[:-1]) + 1 # all the previous nodes\n",
    "                self.nodeops += [IdentityMapping(n_preds * ni, no, stride=stride)]\n",
    "            else:  # inner partition\n",
    "                self.nodeops += [Partition(ni, partitions[i], links, has_groups, Unit, **kwargs)]\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for nodeop in self.nodeops:\n",
    "            if self.efficient:\n",
    "                x = cp.checkpoint(nodeop, x)\n",
    "            else:\n",
    "                x = nodeop(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
